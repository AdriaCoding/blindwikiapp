\section{Estado del Arte}

Antes de la implementación de la aplicación, se ha llevado a cabo un estudio de la literatura para identificar las tecnologías y modelos más relevantes en el desarrollo de aplicaciones móviles multiplataforma, y en las tareas de procesamiento de lenguaje natural y traducción de idiomas anteriormente mencionadas.

\subsection{Desarrollo de Aplicaciones Móviles Multiplataforma}

El desarrollo de aplicaciones multiplataforma ha evolucionado significativamente durante la última década como respuesta a la fragmentación del mercado móvil entre Android (71.9\%) e iOS (27.68\%) \cite{statcounter2025}, y la necesidad de optimizar recursos de desarrollo. El concepto fundamental \textit{"write once, run everywhere"} constituye el núcleo de esta filosofía.

Históricamente, esta tecnología evolucionó desde enfoques basados en WebView (PhoneGap/Cordova) hacia frameworks de compilación a código nativo, hasta las soluciones actuales que equilibran rendimiento nativo con eficiencia de desarrollo. El rendimiento de nativo es de especial interés para el proyecto blind wiki, ya que eso proporciona una mayor compatibilidad con los lectores de pantalla (TalkBack en Android y VoiceOver en iOS).

\subsubsection{Frameworks Considerados}

\paragraph{Flutter (Google)}
Framework que utiliza Dart como lenguaje y un motor de renderizado propio (Skia). Su arquitectura se compone de tres capas: el framework Dart, el motor C/C++ con Skia, y los wrappers específicos de plataforma. Entre sus ventajas, Flutter ofrece un rendimiento similar al nativo con 60 FPS en animaciones complejas, una interfaz de usuario consistente y personalizable en todas las plataformas, y widgets adaptables tanto de Material Design como de Cupertino. Además, cuenta con sistema de árbol semántico con descripciones para cada widget, perfectamente integrado con las APIs nativas de accesibilidad (TalkBack/VoiceOver).

\paragraph{React Native (Meta)}
Utiliza JavaScript y React para crear aplicaciones móviles mediante una arquitectura puente que conecta JavaScript con componentes nativos de la plataforma. Sus ventajas incluyen el aprovechamiento del ecosistema JavaScript/npm, la utilización de componentes UI nativos reales, y la facilidad de transición desde el desarrollo web. Además, React Native ofrece integración directa con los componentes nativos de accesibilidad, heredando automáticamente las mejoras del sistema operativo. No obstante, el puente JavaScript-Nativo puede limitar el rendimiento en comparación con Flutter.


\subsubsection{Análisis Comparativo}
En la siguiente tabla \ref{tab:framework-comparison} se muestra un resumen de las características de los frameworks considerados para el desarrollo de la aplicación blind wiki.
\begin{table}[ht]
    \centering
    \caption{Comparación de Frameworks Multiplataforma}
    \label{tab:framework-comparison}
    \begin{tabular}{|l|c|c|}
        \hline
        \textbf{Característica} & \textbf{Flutter} & \textbf{React Native} \\
        \hline
        Lenguaje & Dart & JavaScript \\
        \hline
        Arquitectura UI & Motor propio & Componentes nativos \\
        \hline
        Rendimiento & Excelente & Bueno \\
        \hline
        Hot Reload & Sí & Sí \\
        \hline
        Curva Aprendizaje & Moderada & Baja (dev JS) \\
        \hline
        Madurez & Media (2017) & Alta (2015) \\
        \hline
        Accesibilidad & Árbol semántico & Componentes nativos \\
        \hline
    \end{tabular}
\end{table}

Para el desarrollo de BlindWiki 2.0, se ha optado por React Native, por su mayor madurez, y por que presenta una curva de aprendizaje más suave para el desarrollador. Las limitaciones en cuanto a al rendimiento de React Native en comparación con Flutter no son críticas para el proyecto, ya que los principales cuellos de botella son las conexiones con el servidor y no el rendimiento interno de la aplicación.

\subsection{Traduccion de Audios}
Para poder desarrollar la feature de traduccion de audios, se ha hecho un análisis dele stado del arte de los modelos de IA de código abierto más relevantes. Estos se dividen en cuatro familias principales, según la tarea que estos realizan.

Hoy en dia, el acceso a modelos de traduccion con rendimiento SOTA está fácilmente al alcance de personas sin conocimientos de programacíon gracias a los productos SaSS de diversas compañías. BlindWiki es un proyecto benéfico, con escasa financiación, por lo que es preciso utilizar modelos de gratuitos o de código abierto. También hemos limitado nuestro análisis a modelos preentrenados, con buenos rendimientos en zero-shot.

\subsubsection{Reconocimiento Automático de Habla (ASR)}

El reconocimiento automático de habla (ASR) es fundamental para convertir las notas de voz de los usuarios de BlindWiki en texto, facilitando así la traducción y el etiquetado posterior.


Nuestro caso de uso encuentra otra limitación, que es que el servidor de backend de blind wiki tine no dispone de GPU, por lo que es preciso restringir nuestra mirada a modelos más ligeros o con optimizaciones para cpu.

\paragraph{Whisper}
Teniendo en cuenta todas esas limitaciones, el modelo Whisper es un buen candidato para el proyecto, ya que tiene soporte para más de ....

El modelo es facilmente accesible desde la librería de transformers de Hugging Face, y está disponible en diversos tamaños, desde X paramaetros hasta Y parametros.

\subsubsection{Identificación de idiomas (LID)}
Los modelos de traducción automática de texto requieren que se les proporcione tanto el idioma de salida, como el idioma de entrada. Por lo tanto, es necesario integrar un modelo de identificación de idiomas (LID) en el pipeline de traducción.


\subsection{Traducción de Texto a Texto (T2TT)}
Una vez identificado el idioma de la nota original, necesitamos traducir su transcripción a todos los idiomas en los que se encuentra disponible la aplicación (ver tabla \ref{tab:supported-languages}). 

Es en este punto de la pipeline donde la eficiencia computacional es más crítica, ya que es preciso ejecutar el módulo de traducción para cada uno de los idiomas soportados. Nuestra aproximación inicial fue usar el modelo NLLB-200 \cite{ott2020nllb}, pero rápidamente nos dimos cuenta de que su uso de RAM (10GB) y tiempo de ejecución (8s por traducción) en el servidor de backend eran absolutamente prohibitivos.



\subsubsection{Especificación de la solución}

El proceso tradicional para traducir audios involucra el uso de varios modelos especializados en cascada. Primero, se implementa un modulo de ASR para obtener las transcripciones del audio, luego se identifica el lenguage de la transcripcion con un módulo de LID, luego se traduce la transcripción a todos los idiomas de destino, y finalmente mediante lectores de pantalla el usuario escucha las traducciones.

Nuestra propuesta de innovación es usar un modelo end-to-end para esta tarea. Si bien es cierto que en la actualidad todos los benchmarks \cite{iwslt-findings} y estudios \cite{etchegoyhen2022cascade}, \cite{Sethiya2025} apuntan que en general la calidad de los textos traducidos es superior con el uso de un esquema en cascada, también se reconoce que el gap es cada vez más estrecho.
No obstante, el motivo de peso que nos lleva a implementar el modelo end-to-end es la capacidad que este tiene de preservar la informacion no-verbal del audio original. Aspectos como la prosodia del hablante, y los sonidos de fondo se pierden invariablemente tras la primera fase de ASR del esquema en cascada. En la actualidad, los modelos end-to-end más avanzados son capaces de replicar la voz del hablante original de forma casi indistinguible.  
Para los usuarios ciegos de blind wiki, esto tiene un interés todavía mayor. Siempre que interactúan con sus dispositivos digitales, ecuchan de forma constante la voz robótica de sus lectores de pantalla. Por eso creemos que escuchar las traducciones con una voz alternativa, más humana, captivará mucho más su atención. 

La decisión final es usar los modelos de Seamless Communication, por su gran versatilidad y cobertura de idiomas. Al ser BlindWiki2.0 un proyecto sin ánimo de lucro, desarrollado por un estudiante, MetaAI nos concedió una copia del modelo closed-source SeamlessExpressive.

Nuestra solución consistirá en realizar las traducciones de audio con SeamlessExpressive cuando el idioma objetivo esté entre los idiomas soportados por el modelo (Inglés, Español, Alemán, Francés, Chino o Italiano), y para el resto de idiomas usar SeamlessM4T.

\subsection{Identificación automática de tags}

